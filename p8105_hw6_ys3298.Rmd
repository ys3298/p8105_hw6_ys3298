---
title: "p8105_hw6_ys3298"
author: "Yimeng Shang(ys3298)"
date: "11/15/2019"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
set.seed(10)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

```

# Problem 1
In this problem, you will analyze data gathered to understand the effects of several variables on a child’s birthweight. 

## load and clean data
```{r}
#load
bw = read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names()

# factor
bw_clean = bw %>% 
  mutate(
    babysex = factor(babysex, levels = c("1", "2"), labels = c("male", "female")),
    malform = factor(malform, levels = c("0", "1"), labels = c("absent", "present")),
    frace = factor(frace, levels = c("1", "2", "3", "4", "8", "9"), labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c("1", "2", "3", "4", "8"), labels = c("White", "Black", "Asian", "Puerto Rican", "Other"))
    ) %>% 
  select(bwt, everything())

# show data
bw_clean

# Check missing value
sum(is.na(bw_clean)) 
```

For missing values, there are `r sum(is.na(bw_clean))` missing values.

## Propose a regression model for birthweight

### Modeling process

#### Full model
```{r}
full_mod = lm(bwt ~ ., data = bw_clean)
summary(full_mod)

full_mod %>% broom::tidy() %>% knitr::kable()
```

Firstly, I put all variables into consideration to establish a full model for linear regression. From the result, I noticed that there are lots of variables is not significant, which means that we can not reject the hypothesis that slopes of these variables is not equal to zero, ie the effect of these variables on the outcome can be ignored. Thus, there are some redundant variables in the full model, which makes it a not so good model.

#### BIC model
```{r}
BIC_mod = step(full_mod, direction = "backward",
                k = log(nrow(bw_clean)), trace = FALSE)
summary(BIC_mod)

BIC_mod %>% broom::tidy() %>% knitr::kable()
```

Because we only have a little knowledge about the study and base on hyphothesis may cause bias and depended too much on s
ubjectivity. So I used data-driven method called Bayesian information criterion. 

BIC is a criterion for model selection among a finite set of models. The models can be tested using corresponding BIC values. Lower BIC value indicates lower penalty terms hence a better model. In R, there's a function 'step' to do this process.

From the result above, baby's sex, baby's head circumference at birth, baby's length at birth, mother's weight at delivery, gestational age in weeks, mother's height, mother's race, mother's pre-pregnancy weight and average nomberof cigarettes smoked per day during pregnancy have significant influence on the baby's birth weight. In detail, female baby, head, length, mother's weight at delivery, gestational age in weeks and mother's height have a positive effect on the baby's weight (baby tend to my heavier with increase of these factors.). And, mothers are Black, Asian, Puerto Rican tend to have less heavy baby. Mother's pre-pregnancy weight and average nomberof cigarettes smoked per day during pregnancy have a negative effect on baby's weight.

### show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.

```{r}
bw_clean %>% 
  add_residuals(BIC_mod) %>% 
  add_predictions(BIC_mod) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point(color = "pink") + geom_line( y = 0, color = "red") +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Residuals vs Fitted Values"
  ) 
```

## Compare your model to two others

Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

### One using length at birth and gestational age as predictors (main effects only)

```{r}
main_mod = lm(bwt ~ blength + gaweeks, data = bw_clean) 

main_mod %>% 
  broom::tidy() %>% knitr::kable()
```

### One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}

interaction_mod = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = bw_clean) 

interaction_mod %>% broom::tidy() %>% knitr::kable()
```

### Cross validation

```{r}
cv_df =
  crossv_mc(bw_clean, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>% 
  mutate(BIC_mod = map(train, ~BIC_mod),
         main_effect = map(train, ~main_mod),
         interaction = map(train, ~interaction_mod)) %>% 
  mutate(rmse_BIC = map2_dbl(BIC_mod, test, ~rmse(model = .x, data = .y)),
         rmse_main= map2_dbl(main_effect, test, ~rmse(model = .x, data = .y)),
         rmse_interaction = map2_dbl(interaction, test, ~rmse(model = .x, data = .y))
         )

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model, alpha = 0.5)) + geom_violin()
```

From the plot, we noticed that the rmse for model only consider the main effects are pretty high. The model consider interactions are relatively high compared with the BIC model, which means the BIC model is good compared to these two. 

# Problem 2

For this problem, we’ll use the 2017 Central Park weather data that we’ve seen elsewhere. The code chunk below (adapted from the course website) will download these data.

```{r}
set.seed(10)
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
```{r}
regression_df = 
  weather_df %>% 
  mutate(tmax_y = tmax, tmin_x = tmin) %>% 
  select(tmax_y, tmin_x) 
```

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:

```{r}
bootstrap_results =
  regression_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax_y ~ tmin_x, data = .x) ),
    results = map(models, broom::tidy),
    variables = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results,variables) 
```

## plot of r^2
```{r}
# plot r^2
bootstrap_results %>% 
  filter(term == "tmin_x") %>% 
  select(r.squared, adj.r.squared) %>% 
  ggplot() + geom_histogram(aes(x = r.squared, y =..density.., alpha = 0.3, fill = "pink")) + geom_density(aes(x = r.squared,y=..density.., alpha = 0.3, color = "pink")) +
  labs(
    title = "Distribution Plot of the Estimate of r^2",
    x = "Estimate of r^2",
    y = "Count")
```

**Describe:** From the plot, we can see the estimated r^2 basically follows a normal distribution, although have a heavier tail on the left.

## CI of r^2
```{r}
# compute 95% CI for r^2
CI_r2 = 
  bootstrap_results %>% 
  filter(term == "tmin_x") %>% 
  pull(r.squared) %>% 
  quantile(c(0.025, 0.975))

CI_r2

bootstrap_results %>% 
  filter(term == "tmin_x") %>% 
  select(r.squared, adj.r.squared) %>% 
  ggplot() + geom_histogram(aes(x = r.squared, y =..density.., alpha = 0.3, fill = "pink")) + geom_density(aes(x = r.squared,y=..density.., alpha = 0.3, color = "pink")) +
  geom_vline(aes(xintercept = CI_r2[[1]]), color = "red") +
  geom_vline(aes(xintercept = CI_r2[[2]]), color = "red") +
  labs(
    title = "Distribution Plot of the Estimate of r^2",
    x = "Estimate of r^2",
    y = "Count") 
```

## plot log(β̂ 0∗β̂ 1)
```{r}
# compute log(β̂ 0∗β̂ 1)
# plot log(β̂ 0∗β̂ 1)

log =
  bootstrap_results %>% 
  select(.id, term, estimate) %>% 
  pivot_wider(
    id_cols = .id,
    values_from = estimate,
    names_from = term
  ) %>% 
  janitor::clean_names() %>% 
  mutate(log_value = log(intercept*tmin_x)) 

log %>% 
  ggplot() + geom_histogram(aes(x = log_value, y =..density.., alpha = 0.3)) + geom_density(aes(x = log_value, y=..density.., alpha = 0.3)) +
  labs(
    title = "Distribution Plot of the Estimate of log(beta0_hat * beta1_hat)",
    x = "Estimate of log(beta0_hat * beta1_hat)",
    y = "Count")

```


**Describe:** From the plot above, we can conclude that the estimated log(β̂ 0∗β̂ 1) basically follows a normal distribution.

## 95% CI log(β̂ 0∗β̂ 1)
```{r}
# compute 95% CI for log(β̂ 0∗β̂ 1)

CI_log = 
  log %>% 
  pull(log_value) %>% 
  quantile(c(0.025, 0.975))

CI_log
```

```{r}
log %>% 
  ggplot() + geom_histogram(aes(x = log_value, y =..density.., alpha = 0.3)) + geom_density(aes(x = log_value, y=..density.., alpha = 0.3)) + 
  geom_vline(aes(xintercept = CI_log[[1]]), color = "red") +
  geom_vline(aes(xintercept = CI_log[[2]]), color = "red") +
  labs(
    title = "Distribution Plot of the Estimate of log(beta0_hat * beta1_hat)",
    x = "Estimate of log(beta0_hat * beta1_hat)",
    y = "Count")
```


